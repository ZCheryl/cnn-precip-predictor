{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('results/train_history_1d_precip_newm/gefs_mos_CNN_0.90_0_100_bestmodel.h5', 'r')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60082a5",
   "metadata": {},
   "source": [
    "### modify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85958b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, ConvLSTM2D, Conv3D\n",
    "\n",
    "def create_classifier(type = 'MLP',\n",
    "                      input_shape = None,\n",
    "                      dropout = None,\n",
    "                      nclass = 1):\n",
    "    model = Sequential()\n",
    "\n",
    "    if type == 'CNN':\n",
    "        model.add(Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "     \n",
    "        model.add(Conv2D(32, kernel_size=(2,2), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=(2,2), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dense(nclass, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Not a valid model type.')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb31e6f",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46be5803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN 0.9 200 0.4 batch size 64\n",
      "before oversampling (3932, 49, 45, 1) (3932,)\n",
      "class weight {0: 0.5545839210155148, 1: 0.5}\n",
      "after oversampling (7090, 49, 45, 1) (7090,)\n",
      "Quantile: 0.90\n",
      " lead: 0\n",
      " model: CNN\n",
      "Training set: ~50.00%\n",
      "Test set: ~10.51%\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 47, 43, 16)        448       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 47, 43, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 23, 21, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 22, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 22, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 11, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 10, 9, 16)         2064      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10, 9, 16)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                20544     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 25,665\n",
      "Trainable params: 25,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.6472 - f1_m: 0.7541\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.29728, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.6472 - f1_m: 0.7543 - val_loss: 0.6867 - val_f1_m: 0.2973\n",
      "Epoch 2/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.6180 - f1_m: 0.8158\n",
      "Epoch 00002: val_f1_m improved from 0.29728 to 0.35617, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 7s 63ms/step - loss: 0.6180 - f1_m: 0.8158 - val_loss: 0.6753 - val_f1_m: 0.3562\n",
      "Epoch 3/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.5862 - f1_m: 0.8579\n",
      "Epoch 00003: val_f1_m improved from 0.35617 to 0.39023, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 7s 64ms/step - loss: 0.5862 - f1_m: 0.8579 - val_loss: 0.6627 - val_f1_m: 0.3902\n",
      "Epoch 4/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.5525 - f1_m: 0.8803\n",
      "Epoch 00004: val_f1_m improved from 0.39023 to 0.42313, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 73ms/step - loss: 0.5525 - f1_m: 0.8803 - val_loss: 0.6484 - val_f1_m: 0.4231\n",
      "Epoch 5/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.5134 - f1_m: 0.8965\n",
      "Epoch 00005: val_f1_m improved from 0.42313 to 0.46543, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.5134 - f1_m: 0.8965 - val_loss: 0.6267 - val_f1_m: 0.4654\n",
      "Epoch 6/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.4748 - f1_m: 0.9062\n",
      "Epoch 00006: val_f1_m improved from 0.46543 to 0.51354, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.4748 - f1_m: 0.9062 - val_loss: 0.6020 - val_f1_m: 0.5135\n",
      "Epoch 7/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.4321 - f1_m: 0.9140\n",
      "Epoch 00007: val_f1_m improved from 0.51354 to 0.54275, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 76ms/step - loss: 0.4321 - f1_m: 0.9140 - val_loss: 0.5717 - val_f1_m: 0.5428\n",
      "Epoch 8/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.3893 - f1_m: 0.9153\n",
      "Epoch 00008: val_f1_m improved from 0.54275 to 0.56677, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.3893 - f1_m: 0.9153 - val_loss: 0.5416 - val_f1_m: 0.5668\n",
      "Epoch 9/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.3528 - f1_m: 0.9174\n",
      "Epoch 00009: val_f1_m improved from 0.56677 to 0.57546, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 75ms/step - loss: 0.3528 - f1_m: 0.9174 - val_loss: 0.5106 - val_f1_m: 0.5755\n",
      "Epoch 10/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.3203 - f1_m: 0.9191\n",
      "Epoch 00010: val_f1_m improved from 0.57546 to 0.58216, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 75ms/step - loss: 0.3203 - f1_m: 0.9191 - val_loss: 0.4845 - val_f1_m: 0.5822\n",
      "Epoch 11/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2929 - f1_m: 0.9207\n",
      "Epoch 00011: val_f1_m improved from 0.58216 to 0.59618, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 75ms/step - loss: 0.2929 - f1_m: 0.9207 - val_loss: 0.4555 - val_f1_m: 0.5962\n",
      "Epoch 12/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2709 - f1_m: 0.9216\n",
      "Epoch 00012: val_f1_m did not improve from 0.59618\n",
      "111/111 [==============================] - 9s 81ms/step - loss: 0.2709 - f1_m: 0.9216 - val_loss: 0.4363 - val_f1_m: 0.5928\n",
      "Epoch 13/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2522 - f1_m: 0.9233\n",
      "Epoch 00013: val_f1_m did not improve from 0.59618\n",
      "111/111 [==============================] - 9s 79ms/step - loss: 0.2522 - f1_m: 0.9233 - val_loss: 0.4163 - val_f1_m: 0.5949\n",
      "Epoch 14/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2376 - f1_m: 0.9223\n",
      "Epoch 00014: val_f1_m did not improve from 0.59618\n",
      "111/111 [==============================] - 9s 79ms/step - loss: 0.2376 - f1_m: 0.9223 - val_loss: 0.4023 - val_f1_m: 0.5931\n",
      "Epoch 15/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2254 - f1_m: 0.9246\n",
      "Epoch 00015: val_f1_m improved from 0.59618 to 0.62354, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 9s 77ms/step - loss: 0.2254 - f1_m: 0.9246 - val_loss: 0.3784 - val_f1_m: 0.6235\n",
      "Epoch 16/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2163 - f1_m: 0.9245\n",
      "Epoch 00016: val_f1_m did not improve from 0.62354\n",
      "111/111 [==============================] - 9s 77ms/step - loss: 0.2163 - f1_m: 0.9245 - val_loss: 0.3693 - val_f1_m: 0.6219\n",
      "Epoch 17/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2085 - f1_m: 0.9256\n",
      "Epoch 00017: val_f1_m did not improve from 0.62354\n",
      "111/111 [==============================] - 8s 76ms/step - loss: 0.2085 - f1_m: 0.9256 - val_loss: 0.3581 - val_f1_m: 0.6219\n",
      "Epoch 18/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.2003 - f1_m: 0.9282\n",
      "Epoch 00018: val_f1_m improved from 0.62354 to 0.63238, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 76ms/step - loss: 0.2003 - f1_m: 0.9282 - val_loss: 0.3472 - val_f1_m: 0.6324\n",
      "Epoch 19/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1966 - f1_m: 0.9298\n",
      "Epoch 00019: val_f1_m improved from 0.63238 to 0.63890, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.1966 - f1_m: 0.9298 - val_loss: 0.3349 - val_f1_m: 0.6389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1911 - f1_m: 0.9284\n",
      "Epoch 00020: val_f1_m did not improve from 0.63890\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1911 - f1_m: 0.9284 - val_loss: 0.3370 - val_f1_m: 0.6211\n",
      "Epoch 21/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1896 - f1_m: 0.9311\n",
      "Epoch 00021: val_f1_m did not improve from 0.63890\n",
      "111/111 [==============================] - 8s 73ms/step - loss: 0.1896 - f1_m: 0.9311 - val_loss: 0.3250 - val_f1_m: 0.6295\n",
      "Epoch 22/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1823 - f1_m: 0.9316\n",
      "Epoch 00022: val_f1_m improved from 0.63890 to 0.64163, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.1823 - f1_m: 0.9316 - val_loss: 0.3163 - val_f1_m: 0.6416\n",
      "Epoch 23/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1793 - f1_m: 0.9322\n",
      "Epoch 00023: val_f1_m did not improve from 0.64163\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.1793 - f1_m: 0.9322 - val_loss: 0.3140 - val_f1_m: 0.6331\n",
      "Epoch 24/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1760 - f1_m: 0.9365\n",
      "Epoch 00024: val_f1_m did not improve from 0.64163\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.1760 - f1_m: 0.9365 - val_loss: 0.3074 - val_f1_m: 0.6361\n",
      "Epoch 25/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1748 - f1_m: 0.9364\n",
      "Epoch 00025: val_f1_m improved from 0.64163 to 0.64696, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1748 - f1_m: 0.9364 - val_loss: 0.2992 - val_f1_m: 0.6470\n",
      "Epoch 26/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1702 - f1_m: 0.9365\n",
      "Epoch 00026: val_f1_m did not improve from 0.64696\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1702 - f1_m: 0.9365 - val_loss: 0.2973 - val_f1_m: 0.6399\n",
      "Epoch 27/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1692 - f1_m: 0.9392\n",
      "Epoch 00027: val_f1_m did not improve from 0.64696\n",
      "111/111 [==============================] - 9s 78ms/step - loss: 0.1692 - f1_m: 0.9392 - val_loss: 0.2941 - val_f1_m: 0.6355\n",
      "Epoch 28/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1676 - f1_m: 0.9378\n",
      "Epoch 00028: val_f1_m improved from 0.64696 to 0.64913, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.1676 - f1_m: 0.9378 - val_loss: 0.2829 - val_f1_m: 0.6491\n",
      "Epoch 29/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1643 - f1_m: 0.9392\n",
      "Epoch 00029: val_f1_m did not improve from 0.64913\n",
      "111/111 [==============================] - 8s 76ms/step - loss: 0.1643 - f1_m: 0.9392 - val_loss: 0.2810 - val_f1_m: 0.6443\n",
      "Epoch 30/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1645 - f1_m: 0.9386\n",
      "Epoch 00030: val_f1_m did not improve from 0.64913\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.1645 - f1_m: 0.9386 - val_loss: 0.2811 - val_f1_m: 0.6368\n",
      "Epoch 31/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1596 - f1_m: 0.9398\n",
      "Epoch 00031: val_f1_m did not improve from 0.64913\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.1596 - f1_m: 0.9398 - val_loss: 0.2794 - val_f1_m: 0.6368\n",
      "Epoch 32/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1592 - f1_m: 0.9424\n",
      "Epoch 00032: val_f1_m did not improve from 0.64913\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.1592 - f1_m: 0.9424 - val_loss: 0.2753 - val_f1_m: 0.6385\n",
      "Epoch 33/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1587 - f1_m: 0.9415\n",
      "Epoch 00033: val_f1_m improved from 0.64913 to 0.65244, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.1587 - f1_m: 0.9415 - val_loss: 0.2673 - val_f1_m: 0.6524\n",
      "Epoch 34/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1579 - f1_m: 0.9407\n",
      "Epoch 00034: val_f1_m did not improve from 0.65244\n",
      "111/111 [==============================] - 8s 73ms/step - loss: 0.1579 - f1_m: 0.9407 - val_loss: 0.2721 - val_f1_m: 0.6380\n",
      "Epoch 35/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1519 - f1_m: 0.9448\n",
      "Epoch 00035: val_f1_m improved from 0.65244 to 0.65698, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1519 - f1_m: 0.9448 - val_loss: 0.2656 - val_f1_m: 0.6570\n",
      "Epoch 36/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1530 - f1_m: 0.9449\n",
      "Epoch 00036: val_f1_m did not improve from 0.65698\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.1530 - f1_m: 0.9449 - val_loss: 0.2691 - val_f1_m: 0.6450\n",
      "Epoch 37/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1527 - f1_m: 0.9467\n",
      "Epoch 00037: val_f1_m improved from 0.65698 to 0.66301, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1527 - f1_m: 0.9467 - val_loss: 0.2555 - val_f1_m: 0.6630\n",
      "Epoch 38/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1504 - f1_m: 0.9440\n",
      "Epoch 00038: val_f1_m did not improve from 0.66301\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.1504 - f1_m: 0.9440 - val_loss: 0.2540 - val_f1_m: 0.6630\n",
      "Epoch 39/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1488 - f1_m: 0.9473\n",
      "Epoch 00039: val_f1_m did not improve from 0.66301\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1488 - f1_m: 0.9473 - val_loss: 0.2559 - val_f1_m: 0.6600\n",
      "Epoch 40/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1469 - f1_m: 0.9474\n",
      "Epoch 00040: val_f1_m did not improve from 0.66301\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1469 - f1_m: 0.9474 - val_loss: 0.2500 - val_f1_m: 0.6615\n",
      "Epoch 41/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1476 - f1_m: 0.9453\n",
      "Epoch 00041: val_f1_m improved from 0.66301 to 0.66972, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1476 - f1_m: 0.9453 - val_loss: 0.2433 - val_f1_m: 0.6697\n",
      "Epoch 42/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1482 - f1_m: 0.9449\n",
      "Epoch 00042: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1482 - f1_m: 0.9449 - val_loss: 0.2498 - val_f1_m: 0.6576\n",
      "Epoch 43/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1425 - f1_m: 0.9482\n",
      "Epoch 00043: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1425 - f1_m: 0.9482 - val_loss: 0.2434 - val_f1_m: 0.6618\n",
      "Epoch 44/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1427 - f1_m: 0.9476\n",
      "Epoch 00044: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1427 - f1_m: 0.9476 - val_loss: 0.2480 - val_f1_m: 0.6564\n",
      "Epoch 45/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1432 - f1_m: 0.9476\n",
      "Epoch 00045: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1432 - f1_m: 0.9476 - val_loss: 0.2430 - val_f1_m: 0.6606\n",
      "Epoch 46/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1418 - f1_m: 0.9497\n",
      "Epoch 00046: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1418 - f1_m: 0.9497 - val_loss: 0.2385 - val_f1_m: 0.6606\n",
      "Epoch 47/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1416 - f1_m: 0.9498\n",
      "Epoch 00047: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1416 - f1_m: 0.9498 - val_loss: 0.2384 - val_f1_m: 0.6621\n",
      "Epoch 48/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1416 - f1_m: 0.9490\n",
      "Epoch 00048: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1416 - f1_m: 0.9490 - val_loss: 0.2401 - val_f1_m: 0.6606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1383 - f1_m: 0.9490\n",
      "Epoch 00049: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1383 - f1_m: 0.9490 - val_loss: 0.2342 - val_f1_m: 0.6635\n",
      "Epoch 50/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1387 - f1_m: 0.9502\n",
      "Epoch 00050: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1387 - f1_m: 0.9502 - val_loss: 0.2339 - val_f1_m: 0.6635\n",
      "Epoch 51/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1384 - f1_m: 0.9503\n",
      "Epoch 00051: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1384 - f1_m: 0.9503 - val_loss: 0.2354 - val_f1_m: 0.6507\n",
      "Epoch 52/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1347 - f1_m: 0.9524\n",
      "Epoch 00052: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1347 - f1_m: 0.9524 - val_loss: 0.2313 - val_f1_m: 0.6567\n",
      "Epoch 53/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1359 - f1_m: 0.9523\n",
      "Epoch 00053: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1359 - f1_m: 0.9523 - val_loss: 0.2334 - val_f1_m: 0.6466\n",
      "Epoch 54/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1337 - f1_m: 0.9522\n",
      "Epoch 00054: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1337 - f1_m: 0.9522 - val_loss: 0.2305 - val_f1_m: 0.6526\n",
      "Epoch 55/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1318 - f1_m: 0.9536\n",
      "Epoch 00055: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1318 - f1_m: 0.9536 - val_loss: 0.2223 - val_f1_m: 0.6682\n",
      "Epoch 56/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1331 - f1_m: 0.9528\n",
      "Epoch 00056: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1337 - f1_m: 0.9520 - val_loss: 0.2275 - val_f1_m: 0.6501\n",
      "Epoch 57/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1315 - f1_m: 0.9534\n",
      "Epoch 00057: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1315 - f1_m: 0.9534 - val_loss: 0.2214 - val_f1_m: 0.6592\n",
      "Epoch 58/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1306 - f1_m: 0.9531\n",
      "Epoch 00058: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1306 - f1_m: 0.9531 - val_loss: 0.2210 - val_f1_m: 0.6601\n",
      "Epoch 59/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1321 - f1_m: 0.9537\n",
      "Epoch 00059: val_f1_m did not improve from 0.66972\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1321 - f1_m: 0.9537 - val_loss: 0.2198 - val_f1_m: 0.6631\n",
      "Epoch 60/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1315 - f1_m: 0.9525\n",
      "Epoch 00060: val_f1_m improved from 0.66972 to 0.68152, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1315 - f1_m: 0.9525 - val_loss: 0.2123 - val_f1_m: 0.6815\n",
      "Epoch 61/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1301 - f1_m: 0.9544\n",
      "Epoch 00061: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1301 - f1_m: 0.9544 - val_loss: 0.2183 - val_f1_m: 0.6596\n",
      "Epoch 62/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1292 - f1_m: 0.9543\n",
      "Epoch 00062: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1292 - f1_m: 0.9543 - val_loss: 0.2148 - val_f1_m: 0.6648\n",
      "Epoch 63/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1265 - f1_m: 0.9553\n",
      "Epoch 00063: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1265 - f1_m: 0.9553 - val_loss: 0.2087 - val_f1_m: 0.6741\n",
      "Epoch 64/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1259 - f1_m: 0.9576\n",
      "Epoch 00064: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1259 - f1_m: 0.9576 - val_loss: 0.2187 - val_f1_m: 0.6531\n",
      "Epoch 65/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1265 - f1_m: 0.9546\n",
      "Epoch 00065: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1265 - f1_m: 0.9546 - val_loss: 0.2137 - val_f1_m: 0.6627\n",
      "Epoch 66/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1290 - f1_m: 0.9541\n",
      "Epoch 00066: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1290 - f1_m: 0.9541 - val_loss: 0.2105 - val_f1_m: 0.6654\n",
      "Epoch 67/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1254 - f1_m: 0.9560\n",
      "Epoch 00067: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 74ms/step - loss: 0.1254 - f1_m: 0.9560 - val_loss: 0.2118 - val_f1_m: 0.6629\n",
      "Epoch 68/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1241 - f1_m: 0.9564\n",
      "Epoch 00068: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1241 - f1_m: 0.9564 - val_loss: 0.2097 - val_f1_m: 0.6629\n",
      "Epoch 69/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1252 - f1_m: 0.9551\n",
      "Epoch 00069: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1252 - f1_m: 0.9551 - val_loss: 0.2104 - val_f1_m: 0.6629\n",
      "Epoch 70/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1234 - f1_m: 0.9577\n",
      "Epoch 00070: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1234 - f1_m: 0.9577 - val_loss: 0.2060 - val_f1_m: 0.6646\n",
      "Epoch 71/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1235 - f1_m: 0.9581\n",
      "Epoch 00071: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1235 - f1_m: 0.9581 - val_loss: 0.2091 - val_f1_m: 0.6637\n",
      "Epoch 72/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1233 - f1_m: 0.9583\n",
      "Epoch 00072: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1233 - f1_m: 0.9583 - val_loss: 0.2026 - val_f1_m: 0.6671\n",
      "Epoch 73/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1218 - f1_m: 0.9571\n",
      "Epoch 00073: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1218 - f1_m: 0.9571 - val_loss: 0.1969 - val_f1_m: 0.6756\n",
      "Epoch 74/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1215 - f1_m: 0.9608\n",
      "Epoch 00074: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1215 - f1_m: 0.9608 - val_loss: 0.2106 - val_f1_m: 0.6539\n",
      "Epoch 75/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1208 - f1_m: 0.9585\n",
      "Epoch 00075: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1208 - f1_m: 0.9585 - val_loss: 0.2003 - val_f1_m: 0.6686\n",
      "Epoch 76/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1205 - f1_m: 0.9580\n",
      "Epoch 00076: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1205 - f1_m: 0.9580 - val_loss: 0.1965 - val_f1_m: 0.6735\n",
      "Epoch 77/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1190 - f1_m: 0.9586\n",
      "Epoch 00077: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1190 - f1_m: 0.9586 - val_loss: 0.2009 - val_f1_m: 0.6637\n",
      "Epoch 78/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1189 - f1_m: 0.9591\n",
      "Epoch 00078: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1192 - f1_m: 0.9587 - val_loss: 0.2007 - val_f1_m: 0.6652\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - ETA: 0s - loss: 0.1188 - f1_m: 0.9588\n",
      "Epoch 00079: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1188 - f1_m: 0.9588 - val_loss: 0.1935 - val_f1_m: 0.6745\n",
      "Epoch 80/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1198 - f1_m: 0.9577\n",
      "Epoch 00080: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1198 - f1_m: 0.9577 - val_loss: 0.1991 - val_f1_m: 0.6687\n",
      "Epoch 81/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1225 - f1_m: 0.9565\n",
      "Epoch 00081: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1225 - f1_m: 0.9565 - val_loss: 0.2007 - val_f1_m: 0.6652\n",
      "Epoch 82/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1190 - f1_m: 0.9584\n",
      "Epoch 00082: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1190 - f1_m: 0.9584 - val_loss: 0.1947 - val_f1_m: 0.6696\n",
      "Epoch 83/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1166 - f1_m: 0.9602\n",
      "Epoch 00083: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1166 - f1_m: 0.9602 - val_loss: 0.1933 - val_f1_m: 0.6696\n",
      "Epoch 84/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1192 - f1_m: 0.9582\n",
      "Epoch 00084: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1192 - f1_m: 0.9582 - val_loss: 0.2050 - val_f1_m: 0.6580\n",
      "Epoch 85/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1165 - f1_m: 0.9593\n",
      "Epoch 00085: val_f1_m did not improve from 0.68152\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1165 - f1_m: 0.9593 - val_loss: 0.1956 - val_f1_m: 0.6696\n",
      "Epoch 86/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1177 - f1_m: 0.9588\n",
      "Epoch 00086: val_f1_m improved from 0.68152 to 0.68526, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1177 - f1_m: 0.9588 - val_loss: 0.1852 - val_f1_m: 0.6853\n",
      "Epoch 87/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1169 - f1_m: 0.9573\n",
      "Epoch 00087: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1169 - f1_m: 0.9573 - val_loss: 0.1916 - val_f1_m: 0.6696\n",
      "Epoch 88/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1156 - f1_m: 0.9610\n",
      "Epoch 00088: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1156 - f1_m: 0.9610 - val_loss: 0.1966 - val_f1_m: 0.6686\n",
      "Epoch 89/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1150 - f1_m: 0.9616\n",
      "Epoch 00089: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.1150 - f1_m: 0.9616 - val_loss: 0.1851 - val_f1_m: 0.6800\n",
      "Epoch 90/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1133 - f1_m: 0.9586\n",
      "Epoch 00090: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1133 - f1_m: 0.9586 - val_loss: 0.1918 - val_f1_m: 0.6696\n",
      "Epoch 91/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1166 - f1_m: 0.9606\n",
      "Epoch 00091: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1166 - f1_m: 0.9606 - val_loss: 0.1885 - val_f1_m: 0.6712\n",
      "Epoch 92/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1167 - f1_m: 0.9594\n",
      "Epoch 00092: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1167 - f1_m: 0.9594 - val_loss: 0.1854 - val_f1_m: 0.6722\n",
      "Epoch 93/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1126 - f1_m: 0.9617\n",
      "Epoch 00093: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1126 - f1_m: 0.9617 - val_loss: 0.1816 - val_f1_m: 0.6764\n",
      "Epoch 94/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1152 - f1_m: 0.9584\n",
      "Epoch 00094: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1152 - f1_m: 0.9584 - val_loss: 0.1894 - val_f1_m: 0.6712\n",
      "Epoch 95/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1134 - f1_m: 0.9607\n",
      "Epoch 00095: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1134 - f1_m: 0.9607 - val_loss: 0.1900 - val_f1_m: 0.6712\n",
      "Epoch 96/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1129 - f1_m: 0.9630\n",
      "Epoch 00096: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1129 - f1_m: 0.9630 - val_loss: 0.1813 - val_f1_m: 0.6764\n",
      "Epoch 97/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1126 - f1_m: 0.9619\n",
      "Epoch 00097: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1126 - f1_m: 0.9619 - val_loss: 0.1833 - val_f1_m: 0.6754\n",
      "Epoch 98/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1107 - f1_m: 0.9627\n",
      "Epoch 00098: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1107 - f1_m: 0.9627 - val_loss: 0.1920 - val_f1_m: 0.6681\n",
      "Epoch 99/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1125 - f1_m: 0.9608\n",
      "Epoch 00099: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1125 - f1_m: 0.9608 - val_loss: 0.1821 - val_f1_m: 0.6754\n",
      "Epoch 100/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1102 - f1_m: 0.9631\n",
      "Epoch 00100: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1102 - f1_m: 0.9631 - val_loss: 0.1859 - val_f1_m: 0.6712\n",
      "Epoch 101/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1113 - f1_m: 0.9612\n",
      "Epoch 00101: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1113 - f1_m: 0.9612 - val_loss: 0.1820 - val_f1_m: 0.6735\n",
      "Epoch 102/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1109 - f1_m: 0.9593\n",
      "Epoch 00102: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1109 - f1_m: 0.9593 - val_loss: 0.1863 - val_f1_m: 0.6714\n",
      "Epoch 103/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1155 - f1_m: 0.9597\n",
      "Epoch 00103: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1155 - f1_m: 0.9600 - val_loss: 0.1752 - val_f1_m: 0.6828\n",
      "Epoch 104/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1101 - f1_m: 0.9617\n",
      "Epoch 00104: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1101 - f1_m: 0.9617 - val_loss: 0.1810 - val_f1_m: 0.6735\n",
      "Epoch 105/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1108 - f1_m: 0.9609\n",
      "Epoch 00105: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1108 - f1_m: 0.9609 - val_loss: 0.1787 - val_f1_m: 0.6735\n",
      "Epoch 106/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1114 - f1_m: 0.9609\n",
      "Epoch 00106: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1114 - f1_m: 0.9609 - val_loss: 0.1813 - val_f1_m: 0.6735\n",
      "Epoch 107/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1097 - f1_m: 0.9629\n",
      "Epoch 00107: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1097 - f1_m: 0.9629 - val_loss: 0.1786 - val_f1_m: 0.6735\n",
      "Epoch 108/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1123 - f1_m: 0.9603\n",
      "Epoch 00108: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1123 - f1_m: 0.9603 - val_loss: 0.1802 - val_f1_m: 0.6735\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - ETA: 0s - loss: 0.1068 - f1_m: 0.9646\n",
      "Epoch 00109: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1068 - f1_m: 0.9646 - val_loss: 0.1799 - val_f1_m: 0.6735\n",
      "Epoch 110/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1067 - f1_m: 0.9620\n",
      "Epoch 00110: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1067 - f1_m: 0.9620 - val_loss: 0.1732 - val_f1_m: 0.6828\n",
      "Epoch 111/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1084 - f1_m: 0.9635\n",
      "Epoch 00111: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1084 - f1_m: 0.9635 - val_loss: 0.1764 - val_f1_m: 0.6735\n",
      "Epoch 112/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1095 - f1_m: 0.9632\n",
      "Epoch 00112: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1095 - f1_m: 0.9632 - val_loss: 0.1758 - val_f1_m: 0.6788\n",
      "Epoch 113/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1103 - f1_m: 0.9640\n",
      "Epoch 00113: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1103 - f1_m: 0.9640 - val_loss: 0.1724 - val_f1_m: 0.6828\n",
      "Epoch 114/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1062 - f1_m: 0.9642\n",
      "Epoch 00114: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1062 - f1_m: 0.9642 - val_loss: 0.1753 - val_f1_m: 0.6788\n",
      "Epoch 115/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1076 - f1_m: 0.9616\n",
      "Epoch 00115: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.1076 - f1_m: 0.9616 - val_loss: 0.1760 - val_f1_m: 0.6788\n",
      "Epoch 116/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1070 - f1_m: 0.9595\n",
      "Epoch 00116: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1070 - f1_m: 0.9595 - val_loss: 0.1787 - val_f1_m: 0.6725\n",
      "Epoch 117/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1087 - f1_m: 0.9630\n",
      "Epoch 00117: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1087 - f1_m: 0.9630 - val_loss: 0.1715 - val_f1_m: 0.6798\n",
      "Epoch 118/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1057 - f1_m: 0.9647\n",
      "Epoch 00118: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1057 - f1_m: 0.9647 - val_loss: 0.1694 - val_f1_m: 0.6810\n",
      "Epoch 119/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1093 - f1_m: 0.9611\n",
      "Epoch 00119: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1093 - f1_m: 0.9611 - val_loss: 0.1694 - val_f1_m: 0.6810\n",
      "Epoch 120/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1065 - f1_m: 0.9640\n",
      "Epoch 00120: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1065 - f1_m: 0.9640 - val_loss: 0.1732 - val_f1_m: 0.6768\n",
      "Epoch 121/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1050 - f1_m: 0.9634\n",
      "Epoch 00121: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1050 - f1_m: 0.9634 - val_loss: 0.1759 - val_f1_m: 0.6735\n",
      "Epoch 122/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1044 - f1_m: 0.9655\n",
      "Epoch 00122: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1044 - f1_m: 0.9655 - val_loss: 0.1733 - val_f1_m: 0.6768\n",
      "Epoch 123/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1058 - f1_m: 0.9624\n",
      "Epoch 00123: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1058 - f1_m: 0.9624 - val_loss: 0.1744 - val_f1_m: 0.6757\n",
      "Epoch 124/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1039 - f1_m: 0.9621\n",
      "Epoch 00124: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1039 - f1_m: 0.9621 - val_loss: 0.1726 - val_f1_m: 0.6789\n",
      "Epoch 125/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1041 - f1_m: 0.9632\n",
      "Epoch 00125: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1041 - f1_m: 0.9632 - val_loss: 0.1717 - val_f1_m: 0.6810\n",
      "Epoch 126/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1046 - f1_m: 0.9616\n",
      "Epoch 00126: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1046 - f1_m: 0.9616 - val_loss: 0.1718 - val_f1_m: 0.6810\n",
      "Epoch 127/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1071 - f1_m: 0.9631\n",
      "Epoch 00127: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1071 - f1_m: 0.9631 - val_loss: 0.1727 - val_f1_m: 0.6810\n",
      "Epoch 128/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1056 - f1_m: 0.9629\n",
      "Epoch 00128: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1056 - f1_m: 0.9629 - val_loss: 0.1786 - val_f1_m: 0.6675\n",
      "Epoch 129/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1046 - f1_m: 0.9618\n",
      "Epoch 00129: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1051 - f1_m: 0.9615 - val_loss: 0.1775 - val_f1_m: 0.6725\n",
      "Epoch 130/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1033 - f1_m: 0.9627\n",
      "Epoch 00130: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1033 - f1_m: 0.9627 - val_loss: 0.1695 - val_f1_m: 0.6820\n",
      "Epoch 131/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1032 - f1_m: 0.9627\n",
      "Epoch 00131: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.1032 - f1_m: 0.9627 - val_loss: 0.1667 - val_f1_m: 0.6832\n",
      "Epoch 132/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1026 - f1_m: 0.9641\n",
      "Epoch 00132: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1026 - f1_m: 0.9641 - val_loss: 0.1677 - val_f1_m: 0.6820\n",
      "Epoch 133/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1042 - f1_m: 0.9609\n",
      "Epoch 00133: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1042 - f1_m: 0.9609 - val_loss: 0.1687 - val_f1_m: 0.6820\n",
      "Epoch 134/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1035 - f1_m: 0.9627\n",
      "Epoch 00134: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1035 - f1_m: 0.9627 - val_loss: 0.1721 - val_f1_m: 0.6771\n",
      "Epoch 135/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1046 - f1_m: 0.9627\n",
      "Epoch 00135: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1046 - f1_m: 0.9627 - val_loss: 0.1668 - val_f1_m: 0.6820\n",
      "Epoch 136/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1037 - f1_m: 0.9634\n",
      "Epoch 00136: val_f1_m did not improve from 0.68526\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1037 - f1_m: 0.9634 - val_loss: 0.1678 - val_f1_m: 0.6810\n",
      "Epoch 137/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1039 - f1_m: 0.9610\n",
      "Epoch 00137: val_f1_m improved from 0.68526 to 0.68574, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1039 - f1_m: 0.9610 - val_loss: 0.1623 - val_f1_m: 0.6857\n",
      "Epoch 138/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1013 - f1_m: 0.9633\n",
      "Epoch 00138: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1013 - f1_m: 0.9633 - val_loss: 0.1647 - val_f1_m: 0.6832\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - ETA: 0s - loss: 0.1027 - f1_m: 0.9618\n",
      "Epoch 00139: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1027 - f1_m: 0.9618 - val_loss: 0.1677 - val_f1_m: 0.6810\n",
      "Epoch 140/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1055 - f1_m: 0.9629\n",
      "Epoch 00140: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.1055 - f1_m: 0.9629 - val_loss: 0.1676 - val_f1_m: 0.6810\n",
      "Epoch 141/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1018 - f1_m: 0.9630\n",
      "Epoch 00141: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1018 - f1_m: 0.9630 - val_loss: 0.1649 - val_f1_m: 0.6832\n",
      "Epoch 142/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1035 - f1_m: 0.9649\n",
      "Epoch 00142: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1035 - f1_m: 0.9649 - val_loss: 0.1620 - val_f1_m: 0.6846\n",
      "Epoch 143/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1021 - f1_m: 0.9639\n",
      "Epoch 00143: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1021 - f1_m: 0.9639 - val_loss: 0.1632 - val_f1_m: 0.6846\n",
      "Epoch 144/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1019 - f1_m: 0.9635\n",
      "Epoch 00144: val_f1_m did not improve from 0.68574\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1019 - f1_m: 0.9635 - val_loss: 0.1629 - val_f1_m: 0.6857\n",
      "Epoch 145/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1049 - f1_m: 0.9612\n",
      "Epoch 00145: val_f1_m improved from 0.68574 to 0.69038, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1049 - f1_m: 0.9612 - val_loss: 0.1597 - val_f1_m: 0.6904\n",
      "Epoch 146/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1023 - f1_m: 0.9638\n",
      "Epoch 00146: val_f1_m did not improve from 0.69038\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.1023 - f1_m: 0.9638 - val_loss: 0.1603 - val_f1_m: 0.6870\n",
      "Epoch 147/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1031 - f1_m: 0.9636\n",
      "Epoch 00147: val_f1_m did not improve from 0.69038\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.1031 - f1_m: 0.9636 - val_loss: 0.1597 - val_f1_m: 0.6904\n",
      "Epoch 148/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0999 - f1_m: 0.9654\n",
      "Epoch 00148: val_f1_m did not improve from 0.69038\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0999 - f1_m: 0.9654 - val_loss: 0.1619 - val_f1_m: 0.6857\n",
      "Epoch 149/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1015 - f1_m: 0.9643\n",
      "Epoch 00149: val_f1_m improved from 0.69038 to 0.69188, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1015 - f1_m: 0.9643 - val_loss: 0.1579 - val_f1_m: 0.6919\n",
      "Epoch 150/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1016 - f1_m: 0.9651\n",
      "Epoch 00150: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.1016 - f1_m: 0.9651 - val_loss: 0.1622 - val_f1_m: 0.6857\n",
      "Epoch 151/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1001 - f1_m: 0.9627\n",
      "Epoch 00151: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1001 - f1_m: 0.9627 - val_loss: 0.1647 - val_f1_m: 0.6807\n",
      "Epoch 152/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1008 - f1_m: 0.9638\n",
      "Epoch 00152: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1008 - f1_m: 0.9638 - val_loss: 0.1614 - val_f1_m: 0.6836\n",
      "Epoch 153/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1005 - f1_m: 0.96 - ETA: 0s - loss: 0.1007 - f1_m: 0.9637\n",
      "Epoch 00153: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.1007 - f1_m: 0.9637 - val_loss: 0.1658 - val_f1_m: 0.6807\n",
      "Epoch 154/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0996 - f1_m: 0.9653\n",
      "Epoch 00154: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0996 - f1_m: 0.9653 - val_loss: 0.1644 - val_f1_m: 0.6818\n",
      "Epoch 155/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0979 - f1_m: 0.9653\n",
      "Epoch 00155: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0979 - f1_m: 0.9653 - val_loss: 0.1607 - val_f1_m: 0.6848\n",
      "Epoch 156/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0998 - f1_m: 0.9661\n",
      "Epoch 00156: val_f1_m did not improve from 0.69188\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0998 - f1_m: 0.9661 - val_loss: 0.1605 - val_f1_m: 0.6861\n",
      "Epoch 157/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.1032 - f1_m: 0.9629\n",
      "Epoch 00157: val_f1_m improved from 0.69188 to 0.69707, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.1032 - f1_m: 0.9629 - val_loss: 0.1547 - val_f1_m: 0.6971\n",
      "Epoch 158/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0971 - f1_m: 0.9658\n",
      "Epoch 00158: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0971 - f1_m: 0.9658 - val_loss: 0.1655 - val_f1_m: 0.6776\n",
      "Epoch 159/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.1011 - f1_m: 0.9651\n",
      "Epoch 00159: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.1016 - f1_m: 0.9647 - val_loss: 0.1652 - val_f1_m: 0.6786\n",
      "Epoch 160/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0976 - f1_m: 0.9612\n",
      "Epoch 00160: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0976 - f1_m: 0.9612 - val_loss: 0.1616 - val_f1_m: 0.6844\n",
      "Epoch 161/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0965 - f1_m: 0.9667\n",
      "Epoch 00161: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0965 - f1_m: 0.9667 - val_loss: 0.1618 - val_f1_m: 0.6823\n",
      "Epoch 162/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0984 - f1_m: 0.9655\n",
      "Epoch 00162: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0984 - f1_m: 0.9655 - val_loss: 0.1534 - val_f1_m: 0.6971\n",
      "Epoch 163/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0982 - f1_m: 0.9639\n",
      "Epoch 00163: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0982 - f1_m: 0.9639 - val_loss: 0.1582 - val_f1_m: 0.6929\n",
      "Epoch 164/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0967 - f1_m: 0.9637\n",
      "Epoch 00164: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0967 - f1_m: 0.9637 - val_loss: 0.1611 - val_f1_m: 0.6844\n",
      "Epoch 165/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0985 - f1_m: 0.9649\n",
      "Epoch 00165: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0985 - f1_m: 0.9649 - val_loss: 0.1581 - val_f1_m: 0.6873\n",
      "Epoch 166/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0976 - f1_m: 0.9652\n",
      "Epoch 00166: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0976 - f1_m: 0.9652 - val_loss: 0.1623 - val_f1_m: 0.6810\n",
      "Epoch 167/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0963 - f1_m: 0.9655\n",
      "Epoch 00167: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0963 - f1_m: 0.9655 - val_loss: 0.1623 - val_f1_m: 0.6810\n",
      "Epoch 168/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0956 - f1_m: 0.9653\n",
      "Epoch 00168: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 72ms/step - loss: 0.0956 - f1_m: 0.9653 - val_loss: 0.1578 - val_f1_m: 0.6865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0926 - f1_m: 0.9686\n",
      "Epoch 00169: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.0926 - f1_m: 0.9686 - val_loss: 0.1579 - val_f1_m: 0.6865\n",
      "Epoch 170/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0958 - f1_m: 0.9637\n",
      "Epoch 00170: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.0958 - f1_m: 0.9637 - val_loss: 0.1596 - val_f1_m: 0.6844\n",
      "Epoch 171/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0965 - f1_m: 0.9649\n",
      "Epoch 00171: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0965 - f1_m: 0.9649 - val_loss: 0.1622 - val_f1_m: 0.6810\n",
      "Epoch 172/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0953 - f1_m: 0.9655\n",
      "Epoch 00172: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0953 - f1_m: 0.9655 - val_loss: 0.1586 - val_f1_m: 0.6844\n",
      "Epoch 173/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0980 - f1_m: 0.9646\n",
      "Epoch 00173: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0984 - f1_m: 0.9646 - val_loss: 0.1582 - val_f1_m: 0.6852\n",
      "Epoch 174/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0961 - f1_m: 0.9633\n",
      "Epoch 00174: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0961 - f1_m: 0.9633 - val_loss: 0.1521 - val_f1_m: 0.6912\n",
      "Epoch 175/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0959 - f1_m: 0.9671\n",
      "Epoch 00175: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0959 - f1_m: 0.9671 - val_loss: 0.1590 - val_f1_m: 0.6844\n",
      "Epoch 176/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0976 - f1_m: 0.9644\n",
      "Epoch 00176: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0976 - f1_m: 0.9644 - val_loss: 0.1545 - val_f1_m: 0.6897\n",
      "Epoch 177/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0971 - f1_m: 0.9659\n",
      "Epoch 00177: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0971 - f1_m: 0.9659 - val_loss: 0.1553 - val_f1_m: 0.6865\n",
      "Epoch 178/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0948 - f1_m: 0.9655\n",
      "Epoch 00178: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 70ms/step - loss: 0.0948 - f1_m: 0.9655 - val_loss: 0.1564 - val_f1_m: 0.6865\n",
      "Epoch 179/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0962 - f1_m: 0.9653\n",
      "Epoch 00179: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0962 - f1_m: 0.9653 - val_loss: 0.1609 - val_f1_m: 0.6823\n",
      "Epoch 180/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0947 - f1_m: 0.9680\n",
      "Epoch 00180: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0947 - f1_m: 0.9680 - val_loss: 0.1585 - val_f1_m: 0.6844\n",
      "Epoch 181/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0956 - f1_m: 0.9659\n",
      "Epoch 00181: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0956 - f1_m: 0.9659 - val_loss: 0.1564 - val_f1_m: 0.6844\n",
      "Epoch 182/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0966 - f1_m: 0.9652\n",
      "Epoch 00182: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0966 - f1_m: 0.9652 - val_loss: 0.1539 - val_f1_m: 0.6932\n",
      "Epoch 183/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0957 - f1_m: 0.9652\n",
      "Epoch 00183: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0957 - f1_m: 0.9652 - val_loss: 0.1546 - val_f1_m: 0.6888\n",
      "Epoch 184/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0932 - f1_m: 0.9682\n",
      "Epoch 00184: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0932 - f1_m: 0.9682 - val_loss: 0.1591 - val_f1_m: 0.6823\n",
      "Epoch 185/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0958 - f1_m: 0.9664\n",
      "Epoch 00185: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0958 - f1_m: 0.9664 - val_loss: 0.1582 - val_f1_m: 0.6852\n",
      "Epoch 186/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0963 - f1_m: 0.9646\n",
      "Epoch 00186: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0963 - f1_m: 0.9646 - val_loss: 0.1529 - val_f1_m: 0.6932\n",
      "Epoch 187/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0956 - f1_m: 0.9643\n",
      "Epoch 00187: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0956 - f1_m: 0.9643 - val_loss: 0.1574 - val_f1_m: 0.6867\n",
      "Epoch 188/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0954 - f1_m: 0.9667\n",
      "Epoch 00188: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0954 - f1_m: 0.9667 - val_loss: 0.1546 - val_f1_m: 0.6908\n",
      "Epoch 189/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0925 - f1_m: 0.9664\n",
      "Epoch 00189: val_f1_m did not improve from 0.69707\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0925 - f1_m: 0.9664 - val_loss: 0.1543 - val_f1_m: 0.6908\n",
      "Epoch 190/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0921 - f1_m: 0.9674\n",
      "Epoch 00190: val_f1_m improved from 0.69707 to 0.69881, saving model to results/tuning\\gefs_mos_CNN_0.90_0_200_bestmodel.h5\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0921 - f1_m: 0.9674 - val_loss: 0.1497 - val_f1_m: 0.6988\n",
      "Epoch 191/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0955 - f1_m: 0.9639\n",
      "Epoch 00191: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0955 - f1_m: 0.9639 - val_loss: 0.1505 - val_f1_m: 0.6988\n",
      "Epoch 192/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0948 - f1_m: 0.9657\n",
      "Epoch 00192: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 7s 68ms/step - loss: 0.0948 - f1_m: 0.9657 - val_loss: 0.1541 - val_f1_m: 0.6888\n",
      "Epoch 193/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0934 - f1_m: 0.9669\n",
      "Epoch 00193: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 8s 69ms/step - loss: 0.0934 - f1_m: 0.9669 - val_loss: 0.1511 - val_f1_m: 0.6988\n",
      "Epoch 194/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0954 - f1_m: 0.9668\n",
      "Epoch 00194: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 8s 71ms/step - loss: 0.0960 - f1_m: 0.9664 - val_loss: 0.1513 - val_f1_m: 0.6969\n",
      "Epoch 195/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0932 - f1_m: 0.9676\n",
      "Epoch 00195: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0932 - f1_m: 0.9676 - val_loss: 0.1548 - val_f1_m: 0.6908\n",
      "Epoch 196/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0945 - f1_m: 0.9680\n",
      "Epoch 00196: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 8s 68ms/step - loss: 0.0945 - f1_m: 0.9680 - val_loss: 0.1497 - val_f1_m: 0.6988\n",
      "Epoch 197/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0915 - f1_m: 0.9671\n",
      "Epoch 00197: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0915 - f1_m: 0.9671 - val_loss: 0.1514 - val_f1_m: 0.6988\n",
      "Epoch 198/200\n",
      "110/111 [============================>.] - ETA: 0s - loss: 0.0904 - f1_m: 0.9675\n",
      "Epoch 00198: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0905 - f1_m: 0.9675 - val_loss: 0.1540 - val_f1_m: 0.6908\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - ETA: 0s - loss: 0.0912 - f1_m: 0.9679\n",
      "Epoch 00199: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 7s 66ms/step - loss: 0.0912 - f1_m: 0.9679 - val_loss: 0.1525 - val_f1_m: 0.6908\n",
      "Epoch 200/200\n",
      "111/111 [==============================] - ETA: 0s - loss: 0.0939 - f1_m: 0.9646\n",
      "Epoch 00200: val_f1_m did not improve from 0.69881\n",
      "111/111 [==============================] - 7s 67ms/step - loss: 0.0939 - f1_m: 0.9646 - val_loss: 0.1545 - val_f1_m: 0.6908\n",
      "Test loss: 0.15447421371936798\n",
      "Test F1: 0.6023125648498535\n",
      "Saved files: results/tuning/gefs_mos_CNN_0.90_0_200\n"
     ]
    }
   ],
   "source": [
    "trial_num = 1\n",
    "model = 'CNN'\n",
    "q = 0.9\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "dropout = 0.4\n",
    "lead = 0\n",
    "\n",
    "print(model, q, epochs,dropout, 'batch size', batch_size)\n",
    "\n",
    "run_experiment(path = 'data/',\n",
    "               model_type = model,\n",
    "               lead = lead, # 0-13 index for 1-14 day leads\n",
    "               q = q,\n",
    "               batch_size = batch_size,\n",
    "               epochs = epochs,\n",
    "               dropout = dropout,\n",
    "               verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9323f0e",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e80b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.optimizers import RMSprop\n",
    "# from metrics import f1_m\n",
    "# from models_og import create_classifier\n",
    "# from models import create_classifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras import backend as K\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e6e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def get_class_weight(y):\n",
    "#  return {0: y.sum() / y.size, 1: 1 - y.sum() / y.size}\n",
    "   return {0: 0.5*y.size/(y.size-y.sum()), 1: 0.5*y.size/y.size}\n",
    "\n",
    "    \n",
    "def run_experiment(path = 'data/',\n",
    "                   model_type = 'CNN',\n",
    "                   lead = 1,\n",
    "                   q = 0.95,\n",
    "                   batch_size = 32,\n",
    "                   epochs = 100,\n",
    "                   dropout = None,\n",
    "                   verbose = False):\n",
    "    \n",
    "    outfile = 'results/tuning/gefs_mos_%s_%0.2f_%d_%d' % (model_type, q, lead, epochs)\n",
    "    fname = 'tensor/gefs_mos_daily_tensor_precip_center.pkl'\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = pickle.load(open(path+fname, 'rb'))\n",
    "    q_val = np.quantile(np.concatenate((y_train, y_test)), q)\n",
    "    y_train = (y_train > q_val).astype(float)\n",
    "    y_test = (y_test > q_val).astype(float)\n",
    "      \n",
    "    x_train = x_train[:,:,:,:,lead]\n",
    "    x_test = x_test[:,:,:,:,lead]\n",
    "    print('before oversampling', x_train.shape, y_train.shape)\n",
    "    print('class weight', get_class_weight(y_train))\n",
    "\n",
    "\n",
    "    # Plain Oversampling with imbleran\n",
    "    ros = RandomOverSampler(random_state = 0)\n",
    "    lat = x_train.shape[1]\n",
    "    lon = x_train.shape[2]\n",
    "    x_train, y_train = ros.fit_resample(x_train.reshape((-1, lat*lon)), y_train)\n",
    "    x_train = x_train.reshape((-1, lat, lon, 1))\n",
    "\n",
    "    print('after oversampling', x_train.shape, y_train.shape)\n",
    "    # print(x_train[0,:,:,:])\n",
    "    \n",
    "    if model_type =='CNN':\n",
    "        # create lat coord\n",
    "        lat_coord = np.linspace(14,62,49)-38\n",
    "        lat_coord[29:] = lat_coord[29:]-4\n",
    "        lat_coord[:24] = np.round(lat_coord[:24]/max(abs(lat_coord)),2)\n",
    "        lat_coord[29:] = np.round(lat_coord[29:]/max(abs(lat_coord)),2)\n",
    "        lat_coord[24:29] = 0\n",
    "        \n",
    "        # create lon coord\n",
    "        # 97 - 141\n",
    "        # 263 - 219\n",
    "        # 240 - 237\n",
    "        lon_coord = np.linspace(219,263,45)-237\n",
    "        lon_coord[22:]=lon_coord[22:]-3\n",
    "        lon_coord[:18] = np.round(lon_coord[:18]/max(abs(lon_coord)),2)\n",
    "        lon_coord[22:] = np.round(lon_coord[22:]/max(abs(lon_coord)),2)\n",
    "        lon_coord[18:22] = 0\n",
    "        lon_coord = -lon_coord\n",
    "        \n",
    "        # end result should be a rectangle matrix of the size 49*45\n",
    "        lat_coord = np.repeat(lat_coord,45).reshape(49,45)\n",
    "        lon_coord = np.transpose(np.repeat(lon_coord,49).reshape(45,49))\n",
    "        \n",
    "        # match size of input to cnn (3932,49,45,3)\n",
    "        \n",
    "        lat_stacked = np.stack([lat_coord for _ in range(x_train.shape[0])],axis=0)\n",
    "        lon_stacked = np.stack([lon_coord for _ in range(x_train.shape[0])],axis=0)\n",
    "        x_train = np.stack((np.squeeze(x_train), lat_stacked, lon_stacked), axis=3)\n",
    "        lat_stacked = lat_stacked[0:x_test.shape[0],:,:]\n",
    "        lon_stacked = lon_stacked[0:x_test.shape[0],:,:]\n",
    "        x_test = np.stack((np.squeeze(x_test), lat_stacked, lon_stacked), axis=3)\n",
    "\n",
    "\n",
    "    input_shape = x_train.shape[1:] # channels last\n",
    "    # print(input_shape)\n",
    "    \n",
    "    model = create_classifier(input_shape = input_shape,\n",
    "                              type = model_type,\n",
    "                              dropout = dropout,\n",
    "                              nclass = 1)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Quantile: %0.2f\\n lead: %d\\n model: %s' % (q, lead, model_type))\n",
    "        print('Training set: ~%0.2f%%' % ((y_train.sum() / y_train.size) * 100))\n",
    "        print('Test set: ~%0.2f%%' % ((y_test.sum() / y_test.size) * 100))\n",
    "        model.summary()\n",
    "        \n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer = opt,\n",
    "#                  optimizer='Adam',#RMSprop(lr=0.0001),\n",
    "                  metrics=[f1_m])\n",
    "    \n",
    "    # callback to save the best model seen during training\n",
    "    mc = ModelCheckpoint(outfile+'_bestmodel.h5',\n",
    "                          monitor='val_f1_m', mode='max', verbose=verbose, \n",
    "                          save_best_only=True)\n",
    "    # patient early stopping callback\n",
    "    es = EarlyStopping(monitor='val_f1_m', mode='max', \n",
    "                       verbose=verbose, patience=100)\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        validation_data=(x_test, y_test),\n",
    "#                        class_weight = get_class_weight(y_train),\n",
    "                        callbacks=[mc, es])\n",
    "    \n",
    "    history.model = None\n",
    "    history = (history.history['loss'],\n",
    "               history.history['f1_m'],\n",
    "               history.history['val_loss'],\n",
    "               history.history['val_f1_m'])\n",
    "    \n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test F1:', score[1])\n",
    "    print('Saved files:', outfile)\n",
    "    pickle.dump(history, open(outfile+'_history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d37d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
